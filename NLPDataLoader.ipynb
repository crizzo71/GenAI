{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBORu/3yCMdnYGf+VyeEGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crizzo71/GenAI/blob/main/NLPDataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XuE6DTOatl3Y"
      },
      "outputs": [],
      "source": [
        "## Installing required libraries\n",
        "!pip install nltk\n",
        "!pip install transformers==4.42.1\n",
        "!pip install sentencepiece\n",
        "!pip install spacy\n",
        "!pip install numpy==1.26.0\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install torch==2.2.2 torchtext==0.17.2\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install portalocker\n",
        "!pip install numpy pandas\n",
        "!pip install numpy scikit-learn"
      ]
    },
    {
      "source": [
        "## Installing required libraries\n",
        "!pip install nltk\n",
        "!pip install transformers==4.42.1\n",
        "!pip install sentencepiece\n",
        "!pip install spacy\n",
        "!pip install numpy==1.26.0\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install torch==2.2.2 torchtext==0.17.2\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install portalocker\n",
        "# Install pandas and scikit-learn after numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Og0WnpURwCaa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDpH7bmAInIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "print(torchtext.__version__)"
      ],
      "metadata": {
        "id": "a6pKpauHukcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
        "import torchtext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "FgtRSOldzWF_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set-up custom data set and data loader in PyTorch\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
        "    \"Fame's a fickle friend, Harry.\",\n",
        "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
        "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
        "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
        "    \"You are awesome!\"]\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "# Create an instance of your custom dataset\n",
        "custom_dataset = CustomDataset(sentences)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 2\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for batch in dataloader:\n",
        "    print(batch)"
      ],
      "metadata": {
        "id": "gD6-f1ZiwNGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eFi-TMP6zlk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create an instance of your custom data set\n",
        "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 2\n",
        "\n",
        "# Create a data loader\n",
        "#dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate through the data loader\n",
        "for batch in dataloader:\n",
        "    print(batch)"
      ],
      "metadata": {
        "id": "PX9tZpTA040a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "sentences = [\n",
        "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
        "    \"Fame's a fickle friend, Harry.\",\n",
        "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
        "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
        "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
        "    \"You are awesome!\"\n",
        "]\n",
        "\n",
        "# Define a custom data set\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, vocab):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.sentences[idx])\n",
        "        # Convert tokens to tensor indices using vocab\n",
        "        tensor_indices = [self.vocab[token] for token in tokens]\n",
        "        return torch.tensor(tensor_indices)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
        "\n",
        "# Create an instance of your custom data set\n",
        "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
        "\n",
        "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
        "print(\"Sample Items:\")\n",
        "for i in range(6):\n",
        "    sample_item = custom_dataset[i]\n",
        "    print(f\"Item {i + 1}: {sample_item}\")"
      ],
      "metadata": {
        "id": "HrZ4afPLzcwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating tensors for custom data set\n",
        "sentences = [\n",
        "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
        "    \"Fame's a fickle friend, Harry.\",\n",
        "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
        "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
        "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
        "    \"You are awesome!\"\n",
        "]\n",
        "\n",
        "# Define a custom data set\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, vocab):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.sentences[idx])\n",
        "        # Convert tokens to tensor indices using vocab\n",
        "        tensor_indices = [self.vocab[token] for token in tokens]\n",
        "        return torch.tensor(tensor_indices)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
        "\n",
        "# Create an instance of your custom data set\n",
        "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
        "\n",
        "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
        "print(\"Sample Items:\")\n",
        "for i in range(6):\n",
        "    sample_item = custom_dataset[i]\n",
        "    print(f\"Item {i + 1}: {sample_item}\")"
      ],
      "metadata": {
        "id": "6ReStb_i2XWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom collate function\n",
        "def collate_fn(batch):\n",
        "    # Pad sequences within the batch to have equal lengths\n",
        "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "    return padded_batch"
      ],
      "metadata": {
        "id": "hbQMVM4e2mzm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data loader with the custom collate function with batch_first=True,\n",
        "dataloader = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# Iterate through the data loader\n",
        "for batch in dataloader:\n",
        "    for row in batch:\n",
        "        for idx in row:\n",
        "            words = [vocab.get_itos()[idx] for idx in row]\n",
        "        print(words)\n",
        ""
      ],
      "metadata": {
        "id": "KZaQOr0uJrhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom collate function\n",
        "def collate_fn_bfFALSE(batch):\n",
        "    # Pad sequences within the batch to have equal lengths\n",
        "    padded_batch = pad_sequence(batch, padding_value=0)\n",
        "    return padded_batch"
      ],
      "metadata": {
        "id": "5IEOGO2CJ6io"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data loader with the custom collate function with batch_first=True,\n",
        "dataloader_bfFALSE = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn_bfFALSE)\n",
        "\n",
        "# Iterate through the data loader\n",
        "for seq in dataloader_bfFALSE:\n",
        "    for row in seq:\n",
        "        #print(row)\n",
        "        words = [vocab.get_itos()[idx] for idx in row]\n",
        "        print(words)"
      ],
      "metadata": {
        "id": "y_3S4_lTJ9k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the data loader with batch_first = TRUE\n",
        "for batch in dataloader:\n",
        "    print(batch)\n",
        "    print(\"Length of sequences in the batch:\",batch.shape[1])"
      ],
      "metadata": {
        "id": "6kvuxUxyKKrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom data set\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]"
      ],
      "metadata": {
        "id": "WllqcP0tKdBV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset=CustomDataset(sentences)"
      ],
      "metadata": {
        "id": "dYd1W1yMKt2Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset[0]\n"
      ],
      "metadata": {
        "id": "2ZFHC1JEKyoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Tokenize each sample in the batch using the specified tokenizer\n",
        "    tensor_batch = []\n",
        "    for sample in batch:\n",
        "        tokens = tokenizer(sample)\n",
        "        # Convert tokens to vocabulary indices and create a tensor for each sample\n",
        "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
        "\n",
        "    # Pad sequences within the batch to have equal lengths using pad_sequence\n",
        "    # batch_first=True ensures that the tensors have shape (batch_size, max_sequence_length)\n",
        "    padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
        "\n",
        "    # Return the padded batch\n",
        "    return padded_batch"
      ],
      "metadata": {
        "id": "hLZ2oiQuLCZ8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data loader for the custom dataset\n",
        "dataloader = DataLoader(\n",
        "    dataset=custom_dataset,   # Custom PyTorch Dataset containing your data\n",
        "    batch_size=batch_size,     # Number of samples in each mini-batch\n",
        "    shuffle=True,              # Shuffle the data at the beginning of each epoch\n",
        "    collate_fn=collate_fn      # Custom collate function for processing batches\n",
        ")"
      ],
      "metadata": {
        "id": "tXKerA7WLH_s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "for batch in dataloader:\n",
        "    print(batch)\n",
        "    print(\"shape of sample\",len(batch))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "98fspL1nLMms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}